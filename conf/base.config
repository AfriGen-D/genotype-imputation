/*
========================================================================================
    Base configuration for imputation workflow
========================================================================================
*/

params {
    // Input/output options
    input                      = null
    outdir                     = './results'
    publish_dir_mode           = 'copy'
    
    // Reference genome options
    genome                     = null
    genome_build               = 'b38' // 'b37' or 'b38'
    reference_panels           = []
    eagle_genetic_map          = null
    reference_genome           = null
    
    // QC options
    qc_min_ac                  = 2
    qc_min_maf                 = 0.01
    qc_max_missing             = 0.05
    qc_hwe_pvalue              = 1e-6
    remove_duplicates          = true
    split_multiallelic         = true
    
    // Phasing options
    phasing_tool               = 'eagle' // 'eagle', 'shapeit', or 'beagle'
    chunk_size                 = 5000000  // 5 Mb chunks
    
    // Imputation options
    imputation_tool            = 'minimac4' // 'minimac4', 'impute5', or 'beagle5'
    impute_info_cutoff         = 0.3
    impute_window              = 500000
    impute_min_ratio           = 0.00001
    impute_ne                  = 20000
    impute_buffer              = 250000
    
    // Reporting options
    report_level               = 'detailed' // 'summary', 'detailed', or 'full'
    generate_plots             = true
    concordance_analysis       = false
    
    // Computational resources
    max_cpus                   = 16
    max_memory                 = '128.GB'
    max_time                   = '240.h'
    
    // Boilerplate options
    help                       = false
    version                    = false
    validate_params            = true
    show_hidden_params         = false
    schema_ignore_params       = 'genomes,modules'
    
    // Config options
    custom_config_version      = 'master'
    custom_config_base         = "https://raw.githubusercontent.com/nf-core/configs/${params.custom_config_version}"
}

// Load modules.config for DSL2 module specific options
includeConfig 'modules.config'

// Workflow introspection
manifest {
    name            = 'genotype-imputation'
    author          = 'H3ABioNet'
    homePage        = 'https://github.com/h3abionet/chipimputation'
    description     = 'Nextflow pipeline for genotype imputation'
    mainScript      = 'main.nf'
    nextflowVersion = '!>=23.04.0'
    version         = '2.0.0'
    doi             = ''
}

// Function to ensure that resource requirements don't go beyond a maximum limit
def check_max(obj, type) {
    if (type == 'memory') {
        try {
            if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
                return params.max_memory as nextflow.util.MemoryUnit
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'cpus') {
        try {
            return Math.min(obj, params.max_cpus as int)
        } catch (all) {
            println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'time') {
        try {
            if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
                return params.max_time as nextflow.util.Duration
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
            return obj
        }
    }
}

// Capture exit codes from upstream processes when piping
process.shell = ['/bin/bash', '-euo', 'pipefail']

// Execution profiles
profiles {
    debug {
        dumpHashes              = true
        process.beforeScript    = 'echo $HOSTNAME'
        cleanup                 = false
        nextflow.enable.configProcessNamesValidation = true
    }
    
    standard {
        docker.enabled          = false
        singularity.enabled     = false
        podman.enabled          = false
        shifter.enabled         = false
        charliecloud.enabled    = false
        conda.enabled           = false
    }
    
    docker {
        docker.enabled          = true
        docker.userEmulation    = true
        singularity.enabled     = false
        podman.enabled          = false
        shifter.enabled         = false
        charliecloud.enabled    = false
        conda.enabled           = false
    }
    
    singularity {
        singularity.enabled     = true
        singularity.autoMounts  = true
        docker.enabled          = false
        podman.enabled          = false
        shifter.enabled         = false
        charliecloud.enabled    = false
        conda.enabled           = false
    }
    
    podman {
        podman.enabled          = true
        docker.enabled          = false
        singularity.enabled     = false
        shifter.enabled         = false
        charliecloud.enabled    = false
        conda.enabled           = false
    }
    
    test {
        includeConfig 'test/test.config'
    }
    
    test_full {
        includeConfig 'test/test_full.config'
    }
}

// Export these variables to prevent local issues
env {
    PYTHONNOUSERSITE = 1
    R_PROFILE_USER   = "/.Rprofile"
    R_ENVIRON_USER   = "/.Renviron"
}

// Capture exit codes
process.shell = ['/bin/bash', '-euo', 'pipefail']

def trace_timestamp = new java.util.Date().format('yyyy-MM-dd_HH-mm-ss')
timeline {
    enabled = true
    file    = "${params.outdir}/pipeline_info/execution_timeline_${trace_timestamp}.html"
}
report {
    enabled = true
    file    = "${params.outdir}/pipeline_info/execution_report_${trace_timestamp}.html"
}
trace {
    enabled = true
    file    = "${params.outdir}/pipeline_info/execution_trace_${trace_timestamp}.txt"
}
dag {
    enabled = true
    file    = "${params.outdir}/pipeline_info/pipeline_dag_${trace_timestamp}.html"
}